{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['astro-ph', 'cond-mat', 'cs', 'econ']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "archives = requests.get(\"https://arxiv.org/archive\")\n",
    "archives = [x[0] for x in archives.json()['archives']]\n",
    "archives[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xl/g3gclj4565v72t7dyksshgjr0000gn/T/ipykernel_69556/3437948500.py:2: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  articles = BeautifulSoup(articles.text, 'lxml')\n"
     ]
    }
   ],
   "source": [
    "articles = requests.get(\"https://arxiv.org/list/cs.AI/1501?show=1000\")\n",
    "articles = BeautifulSoup(articles.text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1501.00601', '1501.00653', '1501.01178', '1501.01239']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = articles.select('dt > span > a:nth-child(1)')\n",
    "ids = [x['href'].split('/')[-1] for x in ids]\n",
    "ids[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(id):\n",
    "    article = requests.get(f'https://arxiv.org/abs/{id}')\n",
    "    article = BeautifulSoup(article.text, 'lxml')\n",
    "\n",
    "    # find block, remove 'Abstract:'\n",
    "    abstract = article.select('#abs > blockquote')\n",
    "    abstract = abstract[0].text.strip()[9:]\n",
    "\n",
    "    scholar = requests.get(f\"https://scholar.google.com/scholar_lookup?arxiv_id={id}\")\n",
    "    scholar = BeautifulSoup(scholar.text, 'lxml')\n",
    "\n",
    "    # find link, remove 'Cited by'\n",
    "    citations = scholar.select('#gs_res_ccl_mid > div > div.gs_ri > div.gs_fl.gs_flb > a:nth-child(3)')\n",
    "    citations = int(citations[0].text.split(' ')[-1])\n",
    "\n",
    "    return abstract, citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_abstract(id):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.get(f'https://arxiv.org/abs/{id}') as response:\n",
    "            article = BeautifulSoup(await response.text(), 'lxml')\n",
    "            # find block, remove 'Abstract:'\n",
    "            abstract = article.select('#abs > blockquote')\n",
    "            abstract = abstract[0].text.strip()[9:]\n",
    "    with open(f'data/a-{id}.json', 'w') as f:\n",
    "        json.dump({'page': str(article), 'abstract': abstract}, f)\n",
    "    return abstract\n",
    "\n",
    "async def get_citations(id):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "    async with aiohttp.ClientSession(trust_env=True, headers=headers) as session:\n",
    "        async with session.get(f\"https://scholar.google.com/scholar_lookup?arxiv_id={id}\") as response:\n",
    "            scholar = BeautifulSoup(await response.text(), 'lxml')\n",
    "            \n",
    "            # find link, remove 'Cited by'\n",
    "            citations = scholar.select('#gs_res_ccl_mid > div > div.gs_ri > div.gs_fl.gs_flb > a:nth-child(3)')\n",
    "            match = re.search(r'Cited by (\\d+)', citations[0].text)\n",
    "            citations = int(match.group(1)) if match else 0\n",
    "\n",
    "    with open(f'data/s-{id}.json', 'w') as f:\n",
    "        json.dump({'page': str(scholar), 'citations': citations}, f)\n",
    "    return citations\n",
    "\n",
    "async def get_data(id):\n",
    "    abstract, citations = await asyncio.gather(get_abstract(id), get_citations(id))\n",
    "    return abstract, citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape(ids):\n",
    "    tasks = [asyncio.create_task(get_data(id)) for id in ids]\n",
    "    return await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = await scrape(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/batch/1501.csv\", \"w\") as f:\n",
    "    for id, (abstract, citations) in zip(ids, data):\n",
    "        f.write(f\"{id},{citations},{abstract}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
